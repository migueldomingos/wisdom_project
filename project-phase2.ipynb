{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Phase 2: Video Dialog "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "#Open Search\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "#Embeddings neighborhood\n",
    "import torch\n",
    "\n",
    "#Contextual embeddings and self-attention\n",
    "import numpy as np\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from transformers import CLIPModel, CLIPTokenizer, CLIPProcessor, LlavaForConditionalGeneration, AutoProcessor\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import av\n",
    "import glob\n",
    "\n",
    "import os\n",
    "import yt_dlp\n",
    "\n",
    "from pathlib import Path\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text-based Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the video captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    processed = {}\n",
    "    for video_id, captions in data.items():\n",
    "        processed[video_id] = {\n",
    "            \"segments\": captions['segments'] if 'segments' in captions else captions,\n",
    "        }\n",
    "    return processed\n",
    "\n",
    "# Load the data\n",
    "val_data1 = load_captions_data('captions/val_1.json')\n",
    "val_data2 = load_captions_data('captions/val_2.json')\n",
    "\n",
    "# Combine dictionaries (preserving video_id as keys)\n",
    "all_captions_data = {**val_data1, **val_data2}\n",
    "\n",
    "pprint(f\"Number of captions: {len(all_captions_data)}\")\n",
    "pprint(f\"Example Captions: {all_captions_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('activity_net.v1-3.min.json', 'r') as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "database = {}\n",
    "\n",
    "for video_id in data['database']:\n",
    "    database[\"v_\" + video_id] = data['database'][video_id]\n",
    "\n",
    "# Create the list with all data, sorted by the number of annotations\n",
    "sorted_database = sorted(\n",
    "    database.items(),\n",
    "    key=lambda x: len(x[1]['annotations']),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Top 10 videos in number of annotations\n",
    "top_videos = dict(sorted_database[:27])\n",
    "\n",
    "pprint(top_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_ids = set(database.keys()) & set(all_captions_data.keys())\n",
    "print(f\"Número de IDs correspondentes: {len(matching_ids)}\")\n",
    "print(f\"IDs no top_videos: {list(top_videos.keys())[:5]}...\")\n",
    "print(f\"IDs em all_captions_data: {list(all_captions_data.keys())[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the final captions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_captions = {}\n",
    "#final_dataset_video = {}\n",
    "\n",
    "# Check and store the captions' of the top 10 videos\n",
    "for video_id in top_videos:\n",
    "    try:\n",
    "        if (all_captions_data[video_id] != None):\n",
    "            final_dataset_captions[video_id] = all_captions_data[video_id]\n",
    "            #final_dataset_video[video_id] = top_videos[video_id]\n",
    "    except Exception as e:\n",
    "        None\n",
    "\n",
    "final_dataset_captions.pop(\"v_PJ72Yl0B1rY\", None) # This video has no available URL\n",
    "#final_dataset_video.pop(\"v_PJ72Yl0B1rY\", None)\n",
    "\n",
    "pprint(final_dataset_captions)\n",
    "pprint(len(final_dataset_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyframe extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(video_url, output_path):\n",
    "    ydl_opts = {\n",
    "        'format': 'mp4',\n",
    "        'outtmpl': output_path,\n",
    "        'quiet': True\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([video_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segment_keyframes(video_path, output_dir, t):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        with av.open(video_path) as container:\n",
    "            stream = container.streams.video[0]\n",
    "            stream.codec_context.skip_frame = \"NONKEY\"\n",
    "            time_base = stream.time_base  # Needed to convert pts to seconds\n",
    "\n",
    "            for frame in container.decode(stream):\n",
    "                timestamp_sec = frame.pts * time_base\n",
    "\n",
    "                i = 0\n",
    "                aux = math.inf\n",
    "                right_ts = -1\n",
    "\n",
    "                for s in t:\n",
    "                    # Code to find the closest timestamp \n",
    "                    start = float(s[0])\n",
    "                    end = float(s[1])\n",
    "\n",
    "                    value = abs(float(timestamp_sec) - start) + abs(float(timestamp_sec) - end)\n",
    "                    if value < aux and start <= float(timestamp_sec) <= end:\n",
    "                        aux = value\n",
    "                        right_ts = i\n",
    "                    i += 1\n",
    "\n",
    "                if t[right_ts][0] <= float(timestamp_sec) <= t[right_ts][1]:\n",
    "                    # Save the frame as an image\n",
    "                    out_path = os.path.join(\n",
    "                        output_dir,\n",
    "                        f\"frame_{float(t[right_ts][0])}_{float(t[right_ts][1])}_{round(float(timestamp_sec), 4)}.jpg\"\n",
    "                    )\n",
    "                    frame.to_image().save(out_path, quality=80)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {video_path}: {e}\")\n",
    "\n",
    "# Base folders\n",
    "video_dir = \"videos\"\n",
    "output_base = \"keyframes\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "processed_count = 0\n",
    "missing_count = 0\n",
    "\n",
    "for video_id, metadata in final_dataset_captions.items():\n",
    "    video_path = os.path.join(video_dir, f\"{video_id}.mp4\")\n",
    "    output_dir = os.path.join(output_base, video_id)\n",
    "    t = final_dataset_captions[video_id]['segments']['timestamps']\n",
    "\n",
    "    if not os.path.exists(video_path):\n",
    "        video_url = top_videos[video_id]['url']\n",
    "        print(f\"[Download] {video_id} → {video_url}\")\n",
    "        download_video(video_url, video_path)\n",
    "\n",
    "    if os.path.exists(video_path):\n",
    "        print(f\"[Processing] Extracting keyframes from: {video_id}\")\n",
    "        extract_segment_keyframes(video_path, output_dir, t)\n",
    "        processed_count += 1\n",
    "    else:\n",
    "        print(f\"[Missing] Could not find video after download: {video_id}\")\n",
    "        missing_count += 1\n",
    "\n",
    "print(\"\\nKeyframe extraction completed.\")\n",
    "print(f\"    Processed videos: {processed_count}\")\n",
    "print(f\"    Missing videos: {missing_count}\")\n",
    "print(f\"    Keyframes saved in: {output_base}/<video_id>/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenSearch connection settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connections to the Open Search Server\n",
    "host = 'api.novasearch.org'\n",
    "port = 443\n",
    "\n",
    "user = 'user09'\n",
    "password = 'grupo09fct'\n",
    "index_name = user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if OpenSearch is up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = (user, password),\n",
    "    use_ssl = True,\n",
    "    url_prefix = 'opensearch_v2',\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    ")\n",
    "\n",
    "if client.indices.exists(index_name):\n",
    "\n",
    "    resp = client.indices.open(index = index_name)\n",
    "    print(resp)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX SETTINGS')\n",
    "    settings = client.indices.get_settings(index = index_name)\n",
    "    pprint(settings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX MAPPINGS')\n",
    "    mappings = client.indices.get_mapping(index = index_name)\n",
    "    pprint(mappings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX #DOCs')\n",
    "    print(client.count(index = index_name))\n",
    "else:\n",
    "    print(\"Index does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.delete(index=index_name, ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the index mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"knn\": True\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"video_id\": {\"type\": \"keyword\"},\n",
    "            \"frame_timestamp\": {\"type\": \"float\"},\n",
    "            \"caption\": {\"type\": \"text\"},\n",
    "            \"caption_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 512,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"innerproduct\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 256,\n",
    "                        \"m\": 48\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"image_clip_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 512,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"innerproduct\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 256,\n",
    "                        \"m\": 48\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    print(\"Index already existed. You may force the new mappings.\")\n",
    "else:        \n",
    "    response = client.indices.create(index_name, body=index_body)\n",
    "    print('\\nCreating index:')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode images and text using CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model.get_image_features(**inputs)\n",
    "        return outputs[0].cpu().numpy()\n",
    "\n",
    "    \n",
    "def encode_text(text):\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model.get_text_features(**inputs)\n",
    "        return outputs[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_clip_data(video_id, frame_timestamp, caption, image_path):\n",
    "    caption_vec = encode_text(caption).tolist()\n",
    "    image_vec = encode_image(image_path).tolist()\n",
    "    \n",
    "    doc = {\n",
    "        \"video_id\": video_id,\n",
    "        \"frame_timestamp\": frame_timestamp,\n",
    "        \"caption\": caption,\n",
    "        \"caption_vector\": caption_vec,\n",
    "        \"image_clip_vector\": image_vec\n",
    "    }\n",
    "    \n",
    "    client.index(index=index_name, body=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the images and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyframes_root = Path(\"./keyframes\")\n",
    "\n",
    "for video_folder in keyframes_root.iterdir():\n",
    "    video_id = video_folder.name\n",
    "\n",
    "    for img in video_folder.glob(\"*.jpg\"):\n",
    "        filename_parts = img.stem.split(\"_\")\n",
    "        start_ts = float(filename_parts[1])\n",
    "        end_ts = float(filename_parts[2])\n",
    "        frame_ts = float(filename_parts[3])\n",
    "\n",
    "        img_path = str(img)\n",
    "\n",
    "        timestamp_array = final_dataset_captions[video_id]['segments']['timestamps']\n",
    "        sentences_array = final_dataset_captions[video_id]['segments']['sentences']\n",
    "        \n",
    "        i = timestamp_array.index([start_ts, end_ts])\n",
    "\n",
    "        sentence = sentences_array[i]\n",
    "\n",
    "        index_clip_data(video_id, frame_ts, sentence, img_path)\n",
    "\n",
    "        print(f\"Indexed: {video_id} {img_path} {timestamp_array[i]} {sentences_array[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyframes_dir = \"keyframes\"\n",
    "\n",
    "def find_closest_frame(video_id, timestamp):\n",
    "    folder = os.path.join(keyframes_dir, video_id)\n",
    "    pattern = os.path.join(folder, f\"frame_*_{timestamp:.4f}.jpg\")\n",
    "    \n",
    "    matches = glob.glob(pattern)\n",
    "    if matches:\n",
    "        return matches[0]  # take first match\n",
    "\n",
    "    # If no exact match, fallback to closest\n",
    "    pattern = os.path.join(folder, f\"frame_*_*.jpg\")\n",
    "    frames = glob.glob(pattern)\n",
    "\n",
    "    # Extract float timestamps and find closest\n",
    "    best_match = None\n",
    "    min_diff = float(\"inf\")\n",
    "    for f in frames:\n",
    "        try:\n",
    "            ts = float(f.split(\"_\")[-1].replace(\".jpg\", \"\"))\n",
    "            diff = abs(ts - timestamp)\n",
    "            if diff < min_diff:\n",
    "                best_match = f\n",
    "                min_diff = diff\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Example (Text → Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"a man surfing\"\n",
    "query_embedding = encode_text(query).tolist()\n",
    "\n",
    "search_query = {\n",
    "    \"size\": 5,\n",
    "    \"_source\": [\"video_id\", \"frame_timestamp\"],\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"image_clip_vector\": {\n",
    "                \"vector\": query_embedding,\n",
    "                \"k\": 5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "response = client.search(index=index_name, body=search_query)\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    video_id = hit[\"_source\"][\"video_id\"]\n",
    "    timestamp = hit[\"_source\"][\"frame_timestamp\"]\n",
    "\n",
    "    image_path = find_closest_frame(video_id, timestamp)\n",
    "    print(f\"Video: {video_id} — Time: {timestamp}s\")\n",
    "    print(f\"Found image: {image_path}\")\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{video_id} @ {timestamp}s\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Image not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Example → Using text and image to make similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"a man surfing on a wave\"\n",
    "query_embedding = encode_text(query).tolist()\n",
    "\n",
    "search_query = {\n",
    "    \"size\": 5,\n",
    "    \"_source\": [\"video_id\", \"frame_timestamp\"],\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                {\n",
    "                    \"knn\": {\n",
    "                        \"image_clip_vector\": {\n",
    "                            \"vector\": query_embedding,\n",
    "                            \"k\": 5\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"knn\": {\n",
    "                        \"caption_vector\": {\n",
    "                            \"vector\": query_embedding,\n",
    "                            \"k\": 5\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "response = client.search(index=index_name, body=search_query)\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    video_id = hit[\"_source\"][\"video_id\"]\n",
    "    timestamp = hit[\"_source\"][\"frame_timestamp\"]\n",
    "\n",
    "    image_path = find_closest_frame(video_id, timestamp)\n",
    "    print(f\"Video: {video_id} — Time: {timestamp}s\")\n",
    "    print(f\"Found image: {image_path}\")\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{video_id} @ {timestamp}s\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Image not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Example (Image → Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding = encode_image(\"./keyframes/v_2ji02dSx1nM/frame_18.71_68.33_62.0621.jpg\")\n",
    "\n",
    "search_query = {\n",
    "    \"size\": 5,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"image_clip_vector\": {\n",
    "                \"vector\": image_embedding,\n",
    "                \"k\": 5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=search_query)\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    video_id = hit[\"_source\"][\"video_id\"]\n",
    "    timestamp = hit[\"_source\"][\"frame_timestamp\"]\n",
    "\n",
    "    image_path = find_closest_frame(video_id, timestamp)\n",
    "    print(f\"Video: {video_id} — Time: {timestamp}s\")\n",
    "    print(f\"Found image: {image_path}\")\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{video_id} @ {timestamp}s\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Image not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Example (Image + Text → Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_combined_query(image_path, text_query, alpha=0.5):\n",
    "    \"\"\"\n",
    "    alpha controls the weighting: 0.0 = only text, 1.0 = only image\n",
    "    \"\"\"\n",
    "    image_vec = encode_image(image_path)\n",
    "    text_vec = encode_text(text_query)\n",
    "    \n",
    "    combined_vec = alpha * image_vec + (1 - alpha) * text_vec\n",
    "    return combined_vec / np.linalg.norm(combined_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_emb = encode_image(\"./keyframes/v_2ji02dSx1nM/frame_18.71_68.33_62.0621.jpg\")\n",
    "#txt_emb = encode_text(\"Men yappin some shit\")\n",
    "\n",
    "#combined_emb = (img_emb / np.linalg.norm(img_emb) + txt_emb / np.linalg.norm(txt_emb)) / 2\n",
    "\n",
    "combined_vec = encode_combined_query(\n",
    "    \"./keyframes/v_2ji02dSx1nM/frame_18.71_68.33_62.0621.jpg\", \n",
    "    \"Men yappin some shit\", \n",
    "    alpha=0.5).tolist()\n",
    "\n",
    "# Prepare the OpenSearch query\n",
    "search_query = {\n",
    "    \"size\": 5,\n",
    "    \"_source\": [\"video_id\", \"frame_timestamp\"],\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                {\n",
    "                    \"knn\": {\n",
    "                        \"image_clip_vector\": {\n",
    "                            \"vector\": combined_vec,\n",
    "                            \"k\": 5\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"knn\": {\n",
    "                        \"caption_vector\": {\n",
    "                            \"vector\": combined_vec,\n",
    "                            \"k\": 5\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=search_query)\n",
    "\n",
    "# Display the matched frames\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    video_id = hit[\"_source\"][\"video_id\"]\n",
    "    timestamp = hit[\"_source\"][\"frame_timestamp\"]\n",
    "\n",
    "    image_path = find_closest_frame(video_id, timestamp)\n",
    "    print(f\"Video: {video_id} — Time: {timestamp}s\")\n",
    "    print(f\"Found image: {image_path}\")\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{video_id} @ {timestamp}s\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Image not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Vision and Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5ae76116324916affa3458f2913faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model and processor\n",
    "#model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,        \n",
    "    low_cpu_mem_usage=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llava(image_path, question, max_tokens=64):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    prompt = f\"<|user|>\\n<image>\\n{question}<|end|>\\n<|assistant|>\"\n",
    "    \n",
    "    # Prepare inputs for CPU and float32\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "\n",
    "    response = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the person doing in this frame?\"\n",
    "frame_path = \"./keyframes/v_2ji02dSx1nM/frame_18.71_68.33_62.0621.jpg\"  # output from OpenSearch + CLIP retrieval\n",
    "\n",
    "response = ask_llava(frame_path, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the person doing in this frame?\"\n",
    "frame_path = \"./keyframes/v_2ji02dSx1nM/frame_18.71_68.33_62.0621.jpg\"  # output from OpenSearch + CLIP retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layers Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", output_hidden_states=True).to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def visualize_token_patch_similarity(image: Image.Image, text: str):\n",
    "    inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Image CLS embedding\n",
    "    image_cls = output.vision_model_output.hidden_states[-1][0][0]\n",
    "    image_embed = model.vision_model.post_layernorm(image_cls.unsqueeze(0))\n",
    "    image_embed = model.visual_projection(image_embed)\n",
    "    image_embed = torch.nn.functional.normalize(image_embed, dim=-1)\n",
    "\n",
    "    # Text token embeddings\n",
    "    text_hidden = output.text_model_output.hidden_states[-1][0]  # (tokens, 768)\n",
    "    text_embed = model.text_projection(text_hidden)              # (tokens, 512)\n",
    "    text_embed = torch.nn.functional.normalize(text_embed, dim=-1)\n",
    "\n",
    "    # Filter out special tokens\n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    valid_indices = [\n",
    "        i for i, tok in enumerate(tokens)\n",
    "        if tok not in tokenizer.all_special_tokens\n",
    "    ]\n",
    "\n",
    "    filtered_tokens = [tokens[i] for i in valid_indices]\n",
    "    filtered_embed = text_embed[valid_indices]\n",
    "\n",
    "    # Compute similarity\n",
    "    similarity = torch.matmul(filtered_embed, image_embed.T).squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # Normalize for color mapping\n",
    "    norm = Normalize(vmin=similarity.min(), vmax=similarity.max())\n",
    "    colors = cm.plasma(norm(similarity))\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, len(filtered_tokens) * 0.5 + 1))\n",
    "    bars = ax.barh(range(len(filtered_tokens)), similarity, color=colors)\n",
    "    ax.set_yticks(range(len(filtered_tokens)))\n",
    "    ax.set_yticklabels(filtered_tokens)\n",
    "    ax.set_xlabel(\"Similarity to Image\")\n",
    "    ax.set_title(\"Token Relevance to Image\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Colorbar\n",
    "    sm = cm.ScalarMappable(cmap='plasma', norm=norm)\n",
    "    sm.set_array([])\n",
    "    fig.colorbar(sm, ax=ax, label=\"Cosine Similarity\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_token_similarity(image: Image.Image, text: str):\n",
    "    # Preprocess inputs\n",
    "    inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Image patch embeddings (remove CLS token)\n",
    "    image_hidden = output.vision_model_output.hidden_states[-1][0][1:]  # (49, 768)\n",
    "    image_embeds = model.vision_model.post_layernorm(image_hidden)\n",
    "    image_embeds = model.visual_projection(image_embeds)                # (49, 512)\n",
    "    image_embeds = torch.nn.functional.normalize(image_embeds, dim=-1)\n",
    "\n",
    "    # Text token embeddings\n",
    "    text_hidden = output.text_model_output.hidden_states[-1][0]         # (tokens, 768)\n",
    "    text_embeds = model.text_projection(text_hidden)                    # (tokens, 512)\n",
    "    text_embeds = torch.nn.functional.normalize(text_embeds, dim=-1)\n",
    "\n",
    "    # Filter special tokens\n",
    "    token_ids = inputs[\"input_ids\"][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    valid_indices = [\n",
    "        i for i, tok in enumerate(tokens)\n",
    "        if tok not in tokenizer.all_special_tokens\n",
    "    ]\n",
    "    tokens = [tokens[i] for i in valid_indices]\n",
    "    text_embeds = text_embeds[valid_indices]\n",
    "\n",
    "    # Compute similarity and reshape to 7×7 grid per token\n",
    "    similarity = torch.matmul(image_embeds, text_embeds.T).detach().cpu().numpy()  # (49, valid_tokens)\n",
    "    patch_grid = similarity.reshape(7, 7, -1)\n",
    "\n",
    "    # Plot image and heatmaps\n",
    "    num_tokens = len(tokens)\n",
    "    fig, axs = plt.subplots(1, num_tokens + 1, figsize=(3.5 * (num_tokens + 1), 6))\n",
    "    norm = Normalize(vmin=similarity.min(), vmax=similarity.max())\n",
    "    cmap = matplotlib.colormaps[\"viridis\"]\n",
    "\n",
    "    # Original image\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    # Token heatmaps with shared colorbar\n",
    "    for i, token in enumerate(tokens):\n",
    "        im = axs[i + 1].imshow(patch_grid[:, :, i], cmap=cmap, norm=norm)\n",
    "        axs[i + 1].set_title(f\"'{token}'\")\n",
    "        axs[i + 1].axis(\"off\")\n",
    "\n",
    "    # Add colorbar to the right\n",
    "    cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=axs[-1], fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Similarity (Cosine)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(frame_path).convert(\"RGB\")  # Use a real image\n",
    "text = \"A man talking\"\n",
    "visualize_token_patch_similarity(image, text)\n",
    "visualize_token_similarity(image, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevancy Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_relevancy_map(image: Image.Image, text: str):\n",
    "    # Preprocess and forward pass\n",
    "    inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Embeddings\n",
    "    image_hidden = output.vision_model_output.hidden_states[-1][0][1:]  # Remove CLS\n",
    "    image_embeds = model.vision_model.post_layernorm(image_hidden)\n",
    "    image_embeds = model.visual_projection(image_embeds)                # → (49, 512)\n",
    "    image_embeds = torch.nn.functional.normalize(image_embeds, dim=-1)\n",
    "\n",
    "    text_hidden = output.text_model_output.hidden_states[-1][0]         # (tokens, 512)\n",
    "    text_embeds = model.text_projection(text_hidden)\n",
    "    text_embeds = torch.nn.functional.normalize(text_embeds, dim=-1)\n",
    "\n",
    "    # Mean text embedding (or use a specific token)\n",
    "    sentence_embed = text_embeds.mean(dim=0)  # (512,)\n",
    "\n",
    "    # Cosine similarity (relevance)\n",
    "    relevance = torch.matmul(image_embeds, sentence_embed).detach().cpu().numpy()  # (49,)\n",
    "    heatmap = relevance.reshape(7, 7)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(heatmap, cmap=\"plasma\")\n",
    "    plt.colorbar(label=\"Relevance to text\")\n",
    "    plt.title(f\"Relevancy Map: '{text}'\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_relevancy_map(image, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Token Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_token_graph(image: Image.Image, text: str):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    full_text = \" \".join(tokens)\n",
    "    \n",
    "    # Original similarity\n",
    "    inputs_full = processor(text=[full_text], images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        full_outputs = model(**inputs_full)\n",
    "        orig_score = full_outputs.logits_per_text[0, 0].item()\n",
    "\n",
    "    effects = []\n",
    "    for i in range(len(tokens)):\n",
    "        reduced_tokens = tokens[:i] + tokens[i+1:]\n",
    "        reduced_text = \" \".join(reduced_tokens)\n",
    "        inputs = processor(text=[reduced_text], images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            new_score = outputs.logits_per_text[0, 0].item()\n",
    "        delta = orig_score - new_score\n",
    "        effects.append(delta)\n",
    "\n",
    "    # Plot causal influence\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(tokens, effects, color=\"tomato\")\n",
    "    plt.title(\"Causal Influence of Each Token on Image Similarity\")\n",
    "    plt.ylabel(\"Change in Similarity\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_token_graph(image, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
