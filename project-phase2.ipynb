{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Phase 2: Video Dialog "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "#Open Search\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "#Embeddings neighborhood\n",
    "import torch\n",
    "\n",
    "#Contextual embeddings and self-attention\n",
    "import numpy as np\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from transformers import CLIPModel, CLIPTokenizer, CLIPProcessor, LlavaForConditionalGeneration, AutoProcessor\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from matplotlib.colors import Normalize\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import av\n",
    "import glob\n",
    "\n",
    "import os\n",
    "import yt_dlp\n",
    "\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from torchvision import transforms\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text-based Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the video captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    processed = {}\n",
    "    for video_id, captions in data.items():\n",
    "        processed[video_id] = {\n",
    "            \"segments\": captions['segments'] if 'segments' in captions else captions,\n",
    "        }\n",
    "    return processed\n",
    "\n",
    "# Load the data\n",
    "val_data1 = load_captions_data('captions/val_1.json')\n",
    "val_data2 = load_captions_data('captions/val_2.json')\n",
    "\n",
    "# Combine dictionaries (preserving video_id as keys)\n",
    "all_captions_data = {**val_data1, **val_data2}\n",
    "\n",
    "pprint(f\"Number of captions: {len(all_captions_data)}\")\n",
    "pprint(f\"Example Captions: {all_captions_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('activity_net.v1-3.min.json', 'r') as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "database = {}\n",
    "\n",
    "for video_id in data['database']:\n",
    "    database[\"v_\" + video_id] = data['database'][video_id]\n",
    "\n",
    "# Create the list with all data, sorted by the number of annotations\n",
    "sorted_database = sorted(\n",
    "    database.items(),\n",
    "    key=lambda x: len(x[1]['annotations']),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Top 10 videos in number of annotations\n",
    "top_videos = dict(sorted_database[:27])\n",
    "\n",
    "pprint(top_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_ids = set(database.keys()) & set(all_captions_data.keys())\n",
    "print(f\"Número de IDs correspondentes: {len(matching_ids)}\")\n",
    "print(f\"IDs no top_videos: {list(top_videos.keys())[:5]}...\")\n",
    "print(f\"IDs em all_captions_data: {list(all_captions_data.keys())[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the final captions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_captions = {}\n",
    "#final_dataset_video = {}\n",
    "\n",
    "# Check and store the captions' of the top 10 videos\n",
    "for video_id in top_videos:\n",
    "    try:\n",
    "        if (all_captions_data[video_id] != None):\n",
    "            final_dataset_captions[video_id] = all_captions_data[video_id]\n",
    "            #final_dataset_video[video_id] = top_videos[video_id]\n",
    "    except Exception as e:\n",
    "        None\n",
    "\n",
    "final_dataset_captions.pop(\"v_PJ72Yl0B1rY\", None) # This video has no available URL\n",
    "#final_dataset_video.pop(\"v_PJ72Yl0B1rY\", None)\n",
    "\n",
    "pprint(final_dataset_captions)\n",
    "pprint(len(final_dataset_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyframe extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(video_url, output_path):\n",
    "    ydl_opts = {\n",
    "        'format': 'mp4',\n",
    "        'outtmpl': output_path,\n",
    "        'quiet': True\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([video_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segment_keyframes(video_path, output_dir, t):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        with av.open(video_path) as container:\n",
    "            stream = container.streams.video[0]\n",
    "            stream.codec_context.skip_frame = \"NONKEY\"\n",
    "            time_base = stream.time_base  # Needed to convert pts to seconds\n",
    "\n",
    "            for frame in container.decode(stream):\n",
    "                timestamp_sec = frame.pts * time_base\n",
    "\n",
    "                i = 0\n",
    "                aux = math.inf\n",
    "                right_ts = -1\n",
    "\n",
    "                for s in t:\n",
    "                    # Code to find the closest timestamp \n",
    "                    start = float(s[0])\n",
    "                    end = float(s[1])\n",
    "\n",
    "                    value = abs(float(timestamp_sec) - start) + abs(float(timestamp_sec) - end)\n",
    "                    if value < aux and start <= float(timestamp_sec) <= end:\n",
    "                        aux = value\n",
    "                        right_ts = i\n",
    "                    i += 1\n",
    "\n",
    "                if t[right_ts][0] <= float(timestamp_sec) <= t[right_ts][1]:\n",
    "                    # Save the frame as an image\n",
    "                    out_path = os.path.join(\n",
    "                        output_dir,\n",
    "                        f\"frame_{float(t[right_ts][0])}_{float(t[right_ts][1])}_{round(float(timestamp_sec), 4)}.jpg\"\n",
    "                    )\n",
    "                    frame.to_image().save(out_path, quality=80)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {video_path}: {e}\")\n",
    "\n",
    "# Base folders\n",
    "video_dir = \"videos\"\n",
    "output_base = \"keyframes\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "processed_count = 0\n",
    "missing_count = 0\n",
    "\n",
    "for video_id, metadata in final_dataset_captions.items():\n",
    "    video_path = os.path.join(video_dir, f\"{video_id}.mp4\")\n",
    "    output_dir = os.path.join(output_base, video_id)\n",
    "    t = final_dataset_captions[video_id]['segments']['timestamps']\n",
    "\n",
    "    if not os.path.exists(video_path):\n",
    "        video_url = top_videos[video_id]['url']\n",
    "        print(f\"[Download] {video_id} → {video_url}\")\n",
    "        download_video(video_url, video_path)\n",
    "\n",
    "    if os.path.exists(video_path):\n",
    "        print(f\"[Processing] Extracting keyframes from: {video_id}\")\n",
    "        extract_segment_keyframes(video_path, output_dir, t)\n",
    "        processed_count += 1\n",
    "    else:\n",
    "        print(f\"[Missing] Could not find video after download: {video_id}\")\n",
    "        missing_count += 1\n",
    "\n",
    "print(\"\\nKeyframe extraction completed.\")\n",
    "print(f\"    Processed videos: {processed_count}\")\n",
    "print(f\"    Missing videos: {missing_count}\")\n",
    "print(f\"    Keyframes saved in: {output_base}/<video_id>/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenSearch connection settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connections to the Open Search Server\n",
    "host = 'api.novasearch.org'\n",
    "port = 443\n",
    "\n",
    "user = 'user09'\n",
    "password = 'grupo09fct'\n",
    "index_name = user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if OpenSearch is up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = (user, password),\n",
    "    use_ssl = True,\n",
    "    url_prefix = 'opensearch_v2',\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    ")\n",
    "\n",
    "if client.indices.exists(index_name):\n",
    "\n",
    "    resp = client.indices.open(index = index_name)\n",
    "    print(resp)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX SETTINGS')\n",
    "    settings = client.indices.get_settings(index = index_name)\n",
    "    pprint(settings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX MAPPINGS')\n",
    "    mappings = client.indices.get_mapping(index = index_name)\n",
    "    pprint(mappings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX #DOCs')\n",
    "    print(client.count(index = index_name))\n",
    "else:\n",
    "    print(\"Index does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.delete(index=index_name, ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the index mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"knn\": True\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"video_id\": {\"type\": \"keyword\"},\n",
    "            \"frame_timestamp\": {\"type\": \"float\"},\n",
    "            \"caption\": {\"type\": \"text\"},\n",
    "            \"caption_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 512,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"innerproduct\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 256,\n",
    "                        \"m\": 48\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"image_clip_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 512,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"innerproduct\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 256,\n",
    "                        \"m\": 48\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    print(\"Index already existed. You may force the new mappings.\")\n",
    "else:        \n",
    "    response = client.indices.create(index_name, body=index_body)\n",
    "    print('\\nCreating index:')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode images and text using CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model.get_image_features(**inputs)\n",
    "        return outputs[0].cpu().numpy()\n",
    "\n",
    "    \n",
    "def encode_text(text):\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model.get_text_features(**inputs)\n",
    "        return outputs[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_clip_data(video_id, frame_timestamp, caption, image_path):\n",
    "    caption_vec = encode_text(caption).tolist()\n",
    "    image_vec = encode_image(image_path).tolist()\n",
    "    \n",
    "    doc = {\n",
    "        \"video_id\": video_id,\n",
    "        \"frame_timestamp\": frame_timestamp,\n",
    "        \"caption\": caption,\n",
    "        \"caption_vector\": caption_vec,\n",
    "        \"image_clip_vector\": image_vec\n",
    "    }\n",
    "    \n",
    "    client.index(index=index_name, body=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the images and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyframes_root = Path(\"./keyframes\")\n",
    "\n",
    "for video_folder in keyframes_root.iterdir():\n",
    "    video_id = video_folder.name\n",
    "\n",
    "    for img in video_folder.glob(\"*.jpg\"):\n",
    "        filename_parts = img.stem.split(\"_\")\n",
    "        start_ts = float(filename_parts[1])\n",
    "        end_ts = float(filename_parts[2])\n",
    "        frame_ts = float(filename_parts[3])\n",
    "\n",
    "        img_path = str(img)\n",
    "\n",
    "        timestamp_array = final_dataset_captions[video_id]['segments']['timestamps']\n",
    "        sentences_array = final_dataset_captions[video_id]['segments']['sentences']\n",
    "        \n",
    "        i = timestamp_array.index([start_ts, end_ts])\n",
    "\n",
    "        sentence = sentences_array[i]\n",
    "\n",
    "        index_clip_data(video_id, frame_ts, sentence, img_path)\n",
    "\n",
    "        print(f\"Indexed: {video_id} {img_path} {timestamp_array[i]} {sentences_array[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyframes_dir = \"keyframes\"\n",
    "\n",
    "def find_closest_frame(video_id, timestamp):\n",
    "    folder = os.path.join(keyframes_dir, video_id)\n",
    "    pattern = os.path.join(folder, f\"frame_*_{timestamp:.4f}.jpg\")\n",
    "    \n",
    "    matches = glob.glob(pattern)\n",
    "    if matches:\n",
    "        return matches[0]  # take first match\n",
    "\n",
    "    # If no exact match, fallback to closest\n",
    "    pattern = os.path.join(folder, f\"frame_*_*.jpg\")\n",
    "    frames = glob.glob(pattern)\n",
    "\n",
    "    # Extract float timestamps and find closest\n",
    "    best_match = None\n",
    "    min_diff = float(\"inf\")\n",
    "    for f in frames:\n",
    "        try:\n",
    "            ts = float(f.split(\"_\")[-1].replace(\".jpg\", \"\"))\n",
    "            diff = abs(ts - timestamp)\n",
    "            if diff < min_diff:\n",
    "                best_match = f\n",
    "                min_diff = diff\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Example (Text → Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"a man surfing\"\n",
    "query_embedding = encode_text(query).tolist()\n",
    "\n",
    "search_query = {\n",
    "    \"size\": 5,\n",
    "    \"_source\": [\"video_id\", \"frame_timestamp\"],\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"image_clip_vector\": {\n",
    "                \"vector\": query_embedding,\n",
    "                \"k\": 5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "response = client.search(index=index_name, body=search_query)\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    video_id = hit[\"_source\"][\"video_id\"]\n",
    "    timestamp = hit[\"_source\"][\"frame_timestamp\"]\n",
    "\n",
    "    image_path = find_closest_frame(video_id, timestamp)\n",
    "    print(f\"Video: {video_id} — Time: {timestamp}s\")\n",
    "    print(f\"Found image: {image_path}\")\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{video_id} @ {timestamp}s\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Image not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Example → Using text and image to make similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"a man surfing on a wave\"\n",
    "query_embedding = encode_text(query).tolist()\n",
    "\n",
    "search_query = {\n",
    "    \"size\": 5,\n",
    "    \"_source\": [\"video_id\", \"frame_timestamp\"],\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                {\n",
    "                    \"knn\": {\n",
    "                        \"image_clip_vector\": {\n",
    "                            \"vector\": query_embedding,\n",
    "                            \"k\": 5\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"knn\": {\n",
    "                        \"caption_vector\": {\n",
    "                            \"vector\": query_embedding,\n",
    "                            \"k\": 5\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "response = client.search(index=index_name, body=search_query)\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    video_id = hit[\"_source\"][\"video_id\"]\n",
    "    timestamp = hit[\"_source\"][\"frame_timestamp\"]\n",
    "\n",
    "    image_path = find_closest_frame(video_id, timestamp)\n",
    "    print(f\"Video: {video_id} — Time: {timestamp}s\")\n",
    "    print(f\"Found image: {image_path}\")\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{video_id} @ {timestamp}s\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Image not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Example (Image → Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding = encode_image(\"./keyframes/v_2ji02dSx1nM/frame_18.71_68.33_62.0621.jpg\")\n",
    "\n",
    "search_query = {\n",
    "    \"size\": 5,\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"image_clip_vector\": {\n",
    "                \"vector\": image_embedding,\n",
    "                \"k\": 5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=search_query)\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    video_id = hit[\"_source\"][\"video_id\"]\n",
    "    timestamp = hit[\"_source\"][\"frame_timestamp\"]\n",
    "\n",
    "    image_path = find_closest_frame(video_id, timestamp)\n",
    "    print(f\"Video: {video_id} — Time: {timestamp}s\")\n",
    "    print(f\"Found image: {image_path}\")\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{video_id} @ {timestamp}s\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Image not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Example (Image + Text → Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_combined_query(image_path, text_query, alpha=0.5):\n",
    "    \"\"\"\n",
    "    alpha controls the weighting: 0.0 = only text, 1.0 = only image\n",
    "    \"\"\"\n",
    "    image_vec = encode_image(image_path)\n",
    "    text_vec = encode_text(text_query)\n",
    "    \n",
    "    combined_vec = alpha * image_vec + (1 - alpha) * text_vec\n",
    "    return combined_vec / np.linalg.norm(combined_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_emb = encode_image(\"./keyframes/v_2ji02dSx1nM/frame_18.71_68.33_62.0621.jpg\")\n",
    "\n",
    "#combined_emb = (img_emb / np.linalg.norm(img_emb) + txt_emb / np.linalg.norm(txt_emb)) / 2\n",
    "\n",
    "combined_vec = encode_combined_query(\n",
    "    \"./keyframes/v_2ji02dSx1nM/frame_18.71_68.33_62.0621.jpg\", \n",
    "    \"Men talking\", \n",
    "    alpha=0.5).tolist()\n",
    "\n",
    "# Prepare the OpenSearch query\n",
    "search_query = {\n",
    "    \"size\": 5,\n",
    "    \"_source\": [\"video_id\", \"frame_timestamp\"],\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                {\n",
    "                    \"knn\": {\n",
    "                        \"image_clip_vector\": {\n",
    "                            \"vector\": combined_vec,\n",
    "                            \"k\": 5\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"knn\": {\n",
    "                        \"caption_vector\": {\n",
    "                            \"vector\": combined_vec,\n",
    "                            \"k\": 5\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=search_query)\n",
    "\n",
    "# Display the matched frames\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    video_id = hit[\"_source\"][\"video_id\"]\n",
    "    timestamp = hit[\"_source\"][\"frame_timestamp\"]\n",
    "\n",
    "    image_path = find_closest_frame(video_id, timestamp)\n",
    "    print(f\"Video: {video_id} — Time: {timestamp}s\")\n",
    "    print(f\"Found image: {image_path}\")\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{video_id} @ {timestamp}s\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Image not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Vision and Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model and processor\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,        \n",
    "    low_cpu_mem_usage=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llava(image_path, question, max_tokens=64):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    prompt = f\"<|user|>\\n<image>\\n{question}<|end|>\\n<|assistant|>\"\n",
    "    \n",
    "    # Prepare inputs for CPU and float32\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "\n",
    "    response = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the person doing in this frame?\"\n",
    "frame_path = \"./keyframes/v_2ji02dSx1nM/frame_18.71_68.33_62.0621.jpg\"  # output from OpenSearch + CLIP retrieval\n",
    "\n",
    "response = ask_llava(frame_path, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language-Vision temporal similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frames_from_dir(video_id, frame_dir=\"./keyframes\"):\n",
    "    frame_path = os.path.join(frame_dir, video_id)\n",
    "    frame_files = sorted(glob.glob(os.path.join(frame_path, \"frame_*.jpg\")))\n",
    "    # Extract timestamp from filename assuming format: frame_0.jpg, frame_2.jpg, etc.\n",
    "    frames = []\n",
    "    for f in frame_files:\n",
    "        ts = int(os.path.basename(f).split(\"_\")[1].split(\".\")[0])\n",
    "        img = Image.open(f).convert(\"RGB\")\n",
    "        frames.append((ts, img))\n",
    "    return frames\n",
    "\n",
    "def compute_clip_similarity(image, caption, model, processor, device=\"cuda\"):\n",
    "    inputs = processor(text=[caption], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_feat = model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "        txt_feat = model.get_text_features(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "    txt_feat /= txt_feat.norm(dim=-1, keepdim=True)\n",
    "    similarity = (img_feat @ txt_feat.T).item()\n",
    "    return similarity\n",
    "\n",
    "def plot_similarity_curves(video_id, captions, clip_model, processor, frame_dir=\"./keyframes\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    frames = load_frames_from_dir(video_id, frame_dir)\n",
    "    times = [ts for ts, _ in frames]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i, caption in enumerate(captions):\n",
    "        sims = []\n",
    "        for ts, img in frames:\n",
    "            sim = compute_clip_similarity(img, caption, clip_model, processor, device)\n",
    "            sims.append(sim)\n",
    "        plt.plot(times, sims, label=f\"Caption {i}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"CLIP Similarity\")\n",
    "    plt.title(f\"CLIP Similarity Over Time - {video_id}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "video_id = \"v_94wjthSzsSQ\"  # replace with your video ID\n",
    "captions = final_dataset_captions[video_id]['segments']['sentences']\n",
    "plot_similarity_curves(video_id, captions, clip_model, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradcam_for_caption(image, caption, model, processor, device=\"cuda\"):\n",
    "    inputs = processor(text=[caption], images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Encode features\n",
    "    with torch.no_grad():\n",
    "        img_feat = model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "        txt_feat = model.get_text_features(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
    "    txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n",
    "    score = (img_feat @ txt_feat.T).squeeze(0)\n",
    "\n",
    "    cam = GradCAM(model=model.vision_model, target_layers=[model.vision_model.encoder.layers[-1]], use_cuda=(device==\"cuda\"))\n",
    "    grayscale_cam = cam(input_tensor=inputs[\"pixel_values\"], targets=[ClassifierOutputTarget(score.item())])[0]\n",
    "\n",
    "    img_np = transforms.ToTensor()(image).permute(1, 2, 0).numpy()\n",
    "    img_np = img_np / img_np.max()\n",
    "    cam_image = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n",
    "    return cam_image\n",
    "\n",
    "def plot_attention_grid(video_id, captions, clip_model, processor, max_frames=10, frame_dir=\"./keyframes\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    frames = load_frames_from_dir(video_id, frame_dir)[:max_frames]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(captions), len(frames), figsize=(2 * len(frames), 2.5 * len(captions)))\n",
    "    for i, caption in enumerate(captions):\n",
    "        for j, (ts, img) in enumerate(frames):\n",
    "            heatmap_img = generate_gradcam_for_caption(img, caption, clip_model, processor, device)\n",
    "            axes[i, j].imshow(heatmap_img)\n",
    "            axes[i, j].axis('off')\n",
    "            if i == 0:\n",
    "                axes[i, j].set_title(f\"{ts}s\", fontsize=8)\n",
    "            if j == 0:\n",
    "                axes[i, j].set_ylabel(f\"Caption {i}\", fontsize=8)\n",
    "    plt.suptitle(f\"Grad-CAM Attention Maps for {video_id}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_attention_grid(video_id, captions, clip_model, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language-Vision contrastive moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrastive_similarity_matrix(video_id, captions, clip_model, processor, frame_dir):\n",
    "    keyframes = load_frames_from_dir(video_id, frame_dir)\n",
    "    if len(keyframes) < len(captions):\n",
    "        print(\"Not enough keyframes for contrastive matrix!\")\n",
    "        return\n",
    "\n",
    "    M = len(captions)\n",
    "    sim_matrix = np.zeros((M, M))\n",
    "    for i in range(M):  # captions\n",
    "        for j in range(M):  # keyframes\n",
    "            sim_matrix[i, j] = compute_clip_similarity(keyframes[j][1], captions[i], clip_model, processor)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    im = plt.imshow(sim_matrix, cmap='viridis', interpolation='nearest')\n",
    "    plt.title(f\"CLIP Contrastive Similarity - {video_id}\")\n",
    "    plt.xlabel(\"Keyframe Index\")\n",
    "    plt.ylabel(\"Caption Index\")\n",
    "    plt.colorbar(im, label=\"Similarity\")\n",
    "\n",
    "    # Annotate each cell\n",
    "    for i in range(M):\n",
    "        for j in range(M):\n",
    "            plt.text(j, i, f\"{sim_matrix[i, j]:.2f}\", ha='center', va='center', color='w', fontsize=8)\n",
    "\n",
    "    plt.xticks(range(M))\n",
    "    plt.yticks(range(M))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "compute_contrastive_similarity_matrix(video_id, captions, clip_model, processor, \"./keyframes/v_94wjthSzsSQ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
