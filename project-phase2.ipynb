{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Phase 2: Video Dialog "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "#Open Search\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "#Embeddings neighborhood\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "#Contextual embeddings and self-attention\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text-based Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the video captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    processed = {}\n",
    "    for video_id, captions in data.items():\n",
    "        processed[video_id] = {\n",
    "            \"segments\": captions['segments'] if 'segments' in captions else captions,\n",
    "        }\n",
    "    return processed\n",
    "\n",
    "# Load the data\n",
    "val_data1 = load_captions_data('captions/val_1.json')\n",
    "val_data2 = load_captions_data('captions/val_2.json')\n",
    "\n",
    "# Combine dictionaries (preserving video_id as keys)\n",
    "all_captions_data = {**val_data1, **val_data2}\n",
    "\n",
    "pprint(f\"Number of captions: {len(all_captions_data)}\")\n",
    "pprint(f\"Example Captions: {all_captions_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('activity_net.v1-3.min.json', 'r') as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "database = {}\n",
    "\n",
    "for video_id in data['database']:\n",
    "    database[\"v_\" + video_id] = data['database'][video_id]\n",
    "\n",
    "# Create the list with all data, sorted by the number of annotations\n",
    "sorted_database = sorted(\n",
    "    database.items(),\n",
    "    key=lambda x: len(x[1]['annotations']),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Top 10 videos in number of annotations\n",
    "top_videos = dict(sorted_database[:27])\n",
    "\n",
    "pprint(top_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_ids = set(database.keys()) & set(all_captions_data.keys())\n",
    "print(f\"NÃºmero de IDs correspondentes: {len(matching_ids)}\")\n",
    "print(f\"IDs no top_videos: {list(top_videos.keys())[:5]}...\")\n",
    "print(f\"IDs em all_captions_data: {list(all_captions_data.keys())[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the final captions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_captions = {}\n",
    "\n",
    "\n",
    "# Check and store the captions' of the top 10 videos\n",
    "for video_id in top_videos:\n",
    "    try:\n",
    "        if (all_captions_data[video_id] != None):\n",
    "            final_dataset_captions[video_id] = all_captions_data[video_id]\n",
    "    except Exception as e:\n",
    "        None\n",
    "\n",
    "final_dataset_captions.pop(\"v_PJ72Yl0B1rY\", None) # This video has no available URL\n",
    "\n",
    "pprint(final_dataset_captions)\n",
    "pprint(len(final_dataset_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyframe extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import av.datasets\n",
    "\n",
    "content = av.datasets.curated(\"pexels/time-lapse-video-of-night-sky-857195.mp4\")\n",
    "with av.open(content) as container:\n",
    "    # Signal that we only want to look at keyframes.\n",
    "    stream = container.streams.video[0]\n",
    "    stream.codec_context.skip_frame = \"NONKEY\"\n",
    "\n",
    "    for i, frame in enumerate(container.decode(stream)):\n",
    "        print(frame)\n",
    "        frame.to_image().save(f\"night-sky.{i:04d}.jpg\", quality=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenSearch connection settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connections to the Open Search Server\n",
    "host = 'api.novasearch.org'\n",
    "port = 443\n",
    "\n",
    "user = ''\n",
    "password = ''\n",
    "index_name = user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if OpenSearch is up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = (user, password),\n",
    "    use_ssl = True,\n",
    "    url_prefix = 'opensearch_v2',\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    ")\n",
    "\n",
    "if client.indices.exists(index_name):\n",
    "\n",
    "    resp = client.indices.open(index = index_name)\n",
    "    print(resp)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX SETTINGS')\n",
    "    settings = client.indices.get_settings(index = index_name)\n",
    "    pprint(settings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX MAPPINGS')\n",
    "    mappings = client.indices.get_mapping(index = index_name)\n",
    "    pprint(mappings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX #DOCs')\n",
    "    print(client.count(index = index_name))\n",
    "else:\n",
    "    print(\"Index does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.delete(index=index_name, ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the index mappings for video captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"number_of_replicas\": 0,\n",
    "            \"number_of_shards\": 4,\n",
    "            \"refresh_interval\": \"-1\",\n",
    "            \"knn\": \"true\"\n",
    "        },\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"strict\",\n",
    "        \"properties\": {\n",
    "            #video_id\n",
    "            \"title\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            #sentences\n",
    "            \"description\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    print(\"Index already exists.\")\n",
    "else:        \n",
    "    response = client.indices.create(index_name, body=index_body)\n",
    "    print('\\nCreating index...')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_id, data in final_dataset_captions.items():\n",
    "    print(f\"Title: {video_id}\")\n",
    "    print(f\"Description: {data['segments']['sentences']}\")\n",
    "\n",
    "for video_id, data in final_dataset_captions.items():\n",
    "    filtered_caption = {\n",
    "        \"title\": video_id,\n",
    "        \"description\": data['segments']['sentences']\n",
    "    }\n",
    "    \n",
    "    resp = client.index(index=index_name, id=video_id, body=filtered_caption)\n",
    "    print(resp['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-based Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.count(index = index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a specific caption\n",
    "\n",
    "qtxt = \"finally gets a spare.\"\n",
    "\n",
    "text_query = {\n",
    "  \"size\": 5,\n",
    "  \"_source\": ['title', 'description'],\n",
    "  \"query\": {\n",
    "    \"multi_match\": {\n",
    "      \"query\": qtxt,\n",
    "      \"fields\": ['description'],\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body=text_query,\n",
    "    index=index_name\n",
    ")\n",
    "\n",
    "print(\"\\nSearch results:\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term-level Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a specific video ID\n",
    "\n",
    "term_query = {\n",
    "    \"size\": 5,\n",
    "    \"_source\": [\"title\", \"description\"],\n",
    "    \"query\": {\n",
    "        \"term\": {\n",
    "            \"title\": \"v_od9EdcDcByA\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body=term_query,\n",
    "    index=index_name\n",
    ")\n",
    "\n",
    "print(\"\\nSearch results:\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a video containing the words \"skate\" and \"tricks\"\n",
    "\n",
    "bool_query = {\n",
    "    \"size\": 5,\n",
    "    \"_source\": [\"title\", \"description\"],\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"match\": {\"description\": \"skate\"}},  # Must contain \"skate\"\n",
    "                {\"match\": {\"description\": \"tricks\"}}  # Must contain \"tricks\"\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body=bool_query,\n",
    "    index=index_name\n",
    ")\n",
    "\n",
    "print(\"\\nSearch results:\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Embeddings Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.delete(index=index_name, ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Index mappings to support k-nn vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"number_of_replicas\": 0,\n",
    "            \"number_of_shards\": 4,\n",
    "            \"refresh_interval\": \"-1\",\n",
    "            \"knn\": True\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"strict\",\n",
    "        \"properties\": {\n",
    "            \"title\": { \"type\": \"keyword\" },\n",
    "            \"description\": { \"type\": \"text\" },\n",
    "            \"sentence_embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"innerproduct\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 256,\n",
    "                        \"m\": 48\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    print(\"Index already existed. You may force the new mappings.\")\n",
    "else:        \n",
    "    response = client.indices.create(index_name, body=index_body)\n",
    "    print('\\nCreating index:')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.count(index = index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Tokenizer utilized for the enconding and creation of the embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take average of all tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the video text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = {}\n",
    "\n",
    "for video_id, data in final_dataset_captions.items():\n",
    "    # Join all sentences to one paragraph-like string\n",
    "    full_description = \" \".join(data['segments']['sentences'])\n",
    "    \n",
    "    embedding = encode(full_description)\n",
    "    \n",
    "    all_embeddings[video_id] = {\n",
    "        \"title\": video_id,\n",
    "        \"description\": data['segments']['sentences'],\n",
    "        \"sentence_embedding\": embedding[0].numpy()\n",
    "    }\n",
    "        \n",
    "    resp = client.index(index=index_name, id=video_id, body=all_embeddings[video_id])\n",
    "    print(resp['result'])\n",
    "\n",
    "    stored = client.get(index=index_name, id=video_id)\n",
    "    print(\"\\nIndexed Document:\")\n",
    "    pprint(stored[\"_source\"])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Save the data to the pickle file\n",
    "with open('all_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_embeddings.pkl', 'rb') as f:\n",
    "    all_embeddings = pickle.load(f)\n",
    "print(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search query for embedding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a specific caption using KNN\n",
    "query = \"finally gets a spare.\"\n",
    "query_emb = encode(query)\n",
    "\n",
    "query_denc = {\n",
    "  'size': 5,\n",
    "  '_source': ['title', 'description'],\n",
    "   \"query\": {\n",
    "        \"knn\": {\n",
    "          \"sentence_embedding\": {\n",
    "            \"vector\": query_emb[0].numpy(),\n",
    "            \"k\": 2\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body = query_denc,\n",
    "    index = index_name\n",
    ")\n",
    "\n",
    "print('\\nSearch results:')\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Constrained Embedding Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding of a query and search in the index using that encoded string \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "query = \"show me moments of a man surfing.\"\n",
    "query_embedding = encode(query)[0].numpy().tolist()\n",
    "\n",
    "def get_nouns_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc if token.pos_ in {\"NOUN\", \"VERB\"}])\n",
    "\n",
    "filter_terms = get_nouns_verbs(query)\n",
    "print(f\"Filter terms: {filter_terms}\")\n",
    "\n",
    "search_body = {\n",
    "    'size': 100,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [{\n",
    "                \"knn\": {\n",
    "                    \"sentence_embedding\": {\n",
    "                        \"vector\": query_embedding,\n",
    "                        \"k\": 1\n",
    "                    }\n",
    "                }\n",
    "            }],\n",
    "            \"filter\": [{\n",
    "                \"match\": {\n",
    "                    \"description\": {\n",
    "                        \"query\": filter_terms,\n",
    "                        \"operator\": \"or\"\n",
    "                    }\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    },\n",
    "    \"_source\": [\"title\", \"description\"]\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=search_body)\n",
    "print(f\"Total documents found: {response['hits']['total']['value']}\")\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Title: {hit['_source']['title']}\")\n",
    "    print(\"Relevant sentences:\")\n",
    "    for sentence in hit['_source']['description']:\n",
    "        if \"surf\" in sentence.lower():\n",
    "            print(f\"  â {sentence}\")\n",
    "        else:\n",
    "            print(f\"  - {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"bokling\"  # Intentional typo for \"bowling\"\n",
    "\n",
    "search_body = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"description\": {\n",
    "                \"query\": query,\n",
    "                \"fuzziness\": \"AUTO\",  # Automatically corrects \"bokling\" to \"bowling\"\n",
    "                \"operator\": \"or\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=search_body)\n",
    "print(f\"Total documents found: {response['hits']['total']['value']}\")\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Title: {hit['_source']['title']}\")\n",
    "    print(\"Full description:\")\n",
    "    for sentence in hit['_source']['description']:\n",
    "        print(f\"  - {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Contextual embeddings and Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"He raises his hands feeling victorious.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained BERT model fine-tuned on MS MARCO for relevance ranking\n",
    "model_path = 'nboost/pt-bert-base-uncased-msmarco'\n",
    "\n",
    "# Initialize tokenizer and model with specific configurations\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)  # Converts text to token IDs\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    output_hidden_states=True,  # Returns all hidden states for analysis\n",
    "    output_attentions=True      # Returns attention weights for interpretability\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    config=config  # Load model with the specified configuration\n",
    ")\n",
    "\n",
    "# Tokenize input sentence with proper formatting for BERT\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    return_tensors='pt',         # Returns PyTorch tensors\n",
    "    add_special_tokens=True,     # Adds [CLS] and [SEP] tokens\n",
    "    max_length=512,              # Truncates to BERT's max length\n",
    "    padding=True,                # Pads sequences to max length\n",
    "    truncation=True              # Truncates longer sequences\n",
    ")\n",
    "\n",
    "# Convert token IDs back to tokens for interpretability\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "input_tokens_list = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "# Run model inference without gradient calculation (faster, less memory)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)  # Returns logits, hidden_states, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The format is as follow:\n",
    "# outputs['hidden_states'][layer_m][0][token_n]\n",
    "layer_m = 12\n",
    "token_n = 1\n",
    "\n",
    "# Get all the embeddings of one layer:\n",
    "output_embeddings = outputs['hidden_states'][layer_m][0]\n",
    "output_embeddings.shape\n",
    "\n",
    "hidden_states = outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scatterplot(data, words):\n",
    "\n",
    "    if data.shape[1] == 2:\n",
    "        twodim = data\n",
    "    else:\n",
    "        pca = PCA()\n",
    "        pca.fit(output_embeddings.detach().numpy())\n",
    "        twodim = pca.transform(data)[:,:2]\n",
    "    \n",
    "    plt.style.use('default') # https://matplotlib.org/3.5.1/gallery/style_sheets/style_sheets_reference.html\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "\n",
    "    return\n",
    "\n",
    "display_scatterplot(output_embeddings.detach().numpy(), input_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 4\n",
    "fig, ax_full = plt.subplots(rows, cols)\n",
    "fig.set_figheight(rows*4)\n",
    "fig.set_figwidth(cols*4+3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "\n",
    "layer = 0\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "       \n",
    "        ax = ax_full[r,c]\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 10})\n",
    "        current_hidden_state = hidden_states[layer][0].detach().numpy()\n",
    "        \n",
    "        if current_hidden_state.shape[1] == 2:\n",
    "            twodim = current_hidden_state\n",
    "        else:\n",
    "            twodim = PCA().fit_transform(current_hidden_state)[:,:2]\n",
    "\n",
    "        plt.style.use('default') # https://matplotlib.org/3.5.1/gallery/style_sheets/style_sheets_reference.html\n",
    "        im = ax.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "        for word, (x,y) in zip(input_tokens_list, twodim):\n",
    "            ax.text(x+0.05, y+0.05, word[1:])\n",
    "        \n",
    "        # Show all ticks and label them with the respective list entries\n",
    "        ax.set_title(\"Layer \" + str(layer))\n",
    "            \n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        layer = layer + 1\n",
    "\n",
    "fig.suptitle(\"Visualization of all output embeddings from all layers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello\"\n",
    "sentence2 = \"bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye bye\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'nboost/pt-bert-base-uncased-msmarco'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path,  output_hidden_states=True, output_attentions=True)  \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "\n",
    "inputs = tokenizer(sentence1, sentence2, return_tensors='pt', add_special_tokens=True, max_length=512, padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "input_tokens_list = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The format is as follow:\n",
    "# outputs['hidden_states'][layer_m][0][token_n]\n",
    "layer_m = 12\n",
    "token_n = 1\n",
    "# Get all the embeddings of one layer:\n",
    "output_embeddings = outputs['hidden_states'][layer_m][0]\n",
    "output_embeddings.shape\n",
    "\n",
    "hidden_states = outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scatterplot(data, words):\n",
    "\n",
    "    if data.shape[1] == 2:\n",
    "        twodim = data\n",
    "    else:\n",
    "        pca = PCA()\n",
    "        pca.fit(output_embeddings.detach().numpy())\n",
    "        twodim = pca.transform(data)[:,:2]\n",
    "    \n",
    "    plt.style.use('default') # https://matplotlib.org/3.5.1/gallery/style_sheets/style_sheets_reference.html\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "\n",
    "    return\n",
    "\n",
    "display_scatterplot(output_embeddings.detach().numpy(), input_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "\n",
    "rows = 3\n",
    "cols = 4\n",
    "fig, ax_full = plt.subplots(rows, cols)\n",
    "fig.set_figheight(rows*4)\n",
    "fig.set_figwidth(cols*4+3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "# Define your axis scale ranges\n",
    "x_range = (-12, 12)\n",
    "y_range = (-6, 6)\n",
    "\n",
    "layer = 0\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "       \n",
    "        ax = ax_full[r, c]\n",
    "        plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "        current_hidden_state = hidden_states[layer][0].detach().numpy()\n",
    "\n",
    "        # Filter out [CLS] and [SEP]\n",
    "        tokens_filtered = []\n",
    "        vectors_filtered = []\n",
    "        for i, token in enumerate(input_tokens_list):\n",
    "            if token not in ['[CLS]', '[SEP]']:\n",
    "                tokens_filtered.append(token)\n",
    "                vectors_filtered.append(current_hidden_state[i])\n",
    "\n",
    "        vectors_filtered = np.array(vectors_filtered)\n",
    "\n",
    "        # Apply PCA if necessary\n",
    "        if vectors_filtered.shape[1] == 2:\n",
    "            twodim = vectors_filtered\n",
    "        else:\n",
    "            twodim = PCA().fit_transform(vectors_filtered)[:, :2]\n",
    "\n",
    "        plt.style.use('default')\n",
    "        ax.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='r')\n",
    "\n",
    "        for word, (x, y) in zip(tokens_filtered, twodim):\n",
    "            ax.text(x + 0.05, y + 0.05, word)\n",
    "\n",
    "        # Set title and fixed axes\n",
    "        ax.set_title(\"Layer \" + str(layer))\n",
    "        ax.set_xlim(x_range)\n",
    "        ax.set_ylim(y_range)\n",
    "\n",
    "        layer += 1\n",
    "\n",
    "fig.suptitle(\"Visualization of all output embeddings from all layers (uniform axis scale)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"He raises his hands feeling victorious.\"\n",
    "question = \"What did he raise?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'nboost/pt-bert-base-uncased-msmarco'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path,  output_hidden_states=True, output_attentions=True)  \n",
    "model = AutoModel.from_pretrained(model_path, config=config)\n",
    "\n",
    "inputs = tokenizer(sentence, question, return_tensors='pt', add_special_tokens=True, max_length=512, padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "attentions = outputs.attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of all the attention heads in one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_indices = [i for i, token in enumerate(tokens) if token == '[SEP]']\n",
    "\n",
    "layer = 11\n",
    "\n",
    "rows = 3\n",
    "cols = 4\n",
    "fig, ax_full = plt.subplots(rows, cols)\n",
    "fig.set_figheight(rows*6)\n",
    "fig.set_figwidth(cols*6+4)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "j = 0\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        ax = ax_full[r, c]\n",
    "\n",
    "        sattention = attentions[layer][0][j].numpy()\n",
    "        sattention = np.flip(sattention, 0)  # flip vertically to match y-axis\n",
    "\n",
    "        im = ax.pcolormesh(sattention, cmap='gnuplot')\n",
    "\n",
    "        ax.set_title(f\"Head {j}\")\n",
    "        ax.set_yticks(np.arange(len(tokens)))\n",
    "        if c == 0:\n",
    "            ax.set_yticklabels(reversed(tokens))\n",
    "            ax.set_ylabel(\"Queries\")\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        ax.set_xticks(np.arange(len(tokens)))\n",
    "        if r == rows - 1:\n",
    "            ax.set_xticklabels(tokens)\n",
    "            ax.set_xlabel(\"Keys\")\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "\n",
    "        # Add vertical and horizontal lines at [SEP] positions\n",
    "        for sep in sep_indices:\n",
    "            # Since we flipped the matrix vertically, y index needs to be (len - sep - 1)\n",
    "            ax.axvline(sep, color='white', linestyle='--', linewidth=1)\n",
    "            ax.axhline(len(tokens) - sep - 1, color='white', linestyle='--', linewidth=1)\n",
    "\n",
    "        j += 1\n",
    "\n",
    "fig.suptitle(\"Layer \" + str(layer) + \" Multi-head Self-attentions\")\n",
    "cbar = fig.colorbar(im, ax=ax_full, location='right', shrink=0.5)\n",
    "cbar.ax.set_ylabel(\"Selt-attention\", rotation=-90, va=\"bottom\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Cross-Encoder two times, one for each sentence, and then we compare the similarities between the words\n",
    "inputs_a = tokenizer(question, return_tensors=\"pt\")\n",
    "inputs_b = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_a = model(**inputs_a).last_hidden_state.squeeze(0)\n",
    "    output_b = model(**inputs_b).last_hidden_state.squeeze(0) \n",
    "\n",
    "output_a_norm = torch.nn.functional.normalize(output_a, p=2, dim=1)\n",
    "output_b_norm = torch.nn.functional.normalize(output_b, p=2, dim=1)\n",
    "\n",
    "similarity_matrix = torch.matmul(output_a_norm, output_b_norm.T)  # (seq_len_a, seq_len_b)\n",
    "\n",
    "tokens_a = tokenizer.convert_ids_to_tokens(inputs_a[\"input_ids\"][0])\n",
    "tokens_b = tokenizer.convert_ids_to_tokens(inputs_b[\"input_ids\"][0])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(similarity_matrix.numpy(), xticklabels=tokens_b, yticklabels=tokens_a, cmap=\"viridis\", annot=True, fmt=\".2f\")\n",
    "plt.xlabel(\"Sentence Tokens\")\n",
    "plt.ylabel(\"Question Tokens\")\n",
    "plt.title(\"Similarity between Question and Sentence Tokens\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 11  # analyze the final layer\n",
    "attentions = outputs.attentions  # shape: (num_layers, batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "# Get shape info\n",
    "num_layers = len(attentions)\n",
    "seq_len = attentions[0].shape[-1]\n",
    "\n",
    "# Store total attention received for each token across all heads, per layer\n",
    "attention_received = torch.zeros((num_layers, seq_len))\n",
    "\n",
    "# Loop through each layer\n",
    "for l in range(num_layers):\n",
    "    attn = attentions[l][0]  # shape: (num_heads, seq_len, seq_len)\n",
    "    attn_sum = attn.sum(dim=0)  # sum over query dimension â shape: (seq_len, seq_len)\n",
    "    attention_received[l] = attn_sum.sum(dim=0)  # sum over key dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for token_idx in range(seq_len):\n",
    "    plt.plot(range(num_layers), attention_received[:, token_idx], label=tokens[token_idx])\n",
    "\n",
    "plt.title(\"Total Attention Received by Each Token Across Layers\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Total Attention Received\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
