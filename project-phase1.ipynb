{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Phase 1: Video Dialog "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "#Open Search\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearchpy import helpers\n",
    "\n",
    "#Embeddings neighborhood\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "#Contextual embeddings and self-attention\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "#%matplotlib notebook\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text-based Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the video captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    processed = {}\n",
    "    for video_id, captions in data.items():\n",
    "        processed[video_id] = {\n",
    "            \"segments\": captions['segments'] if 'segments' in captions else captions,\n",
    "        }\n",
    "    return processed\n",
    "\n",
    "# Load the data\n",
    "val_data1 = load_captions_data('captions/val_1.json')\n",
    "val_data2 = load_captions_data('captions/val_2.json')\n",
    "\n",
    "# Combine dictionaries (preserving video_id as keys)\n",
    "all_captions_data = {**val_data1, **val_data2}\n",
    "\n",
    "pprint(f\"Number of captions: {len(all_captions_data)}\")\n",
    "pprint(f\"Example Captions: {all_captions_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('activity_net.v1-3.min.json', 'r') as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "database = {}\n",
    "\n",
    "for video_id in data['database']:\n",
    "    database[\"v_\" + video_id] = data['database'][video_id]\n",
    "\n",
    "# Criar lista ordenada com todos os dados completos\n",
    "sorted_database = sorted(\n",
    "    database.items(),\n",
    "    key=lambda x: len(x[1]['annotations']),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Top 10 videos \n",
    "top_videos = dict(sorted_database[:27])\n",
    "\n",
    "pprint(top_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_ids = set(database.keys()) & set(all_captions_data.keys())\n",
    "print(f\"Número de IDs correspondentes: {len(matching_ids)}\")\n",
    "print(f\"IDs no top_videos: {list(top_videos.keys())[:5]}...\")\n",
    "print(f\"IDs em all_captions_data: {list(all_captions_data.keys())[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the final captions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_captions = {}\n",
    "\n",
    "for video_id in top_videos:\n",
    "    try:\n",
    "        if (all_captions_data[video_id] != None):\n",
    "            #final_dataset_video[video_id] = top_videos[video_id]\n",
    "            final_dataset_captions[video_id] = all_captions_data[video_id]\n",
    "    except Exception as e:\n",
    "        None\n",
    "\n",
    "final_dataset_captions.pop(\"v_PJ72Yl0B1rY\", None)\n",
    "\n",
    "pprint(final_dataset_captions)\n",
    "pprint(len(final_dataset_captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenSearch connection settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost'\n",
    "port = 9200\n",
    "\n",
    "user = 'admin' # Add your user name here.\n",
    "password = 'grupo09FTC!' # Add your user password here. For testing only. Don't store credentials in code. \n",
    "index_name = user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if OpenSearch is up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': port}],\n",
    "    http_auth=(user, password),\n",
    "    use_ssl=True,              \n",
    "    verify_certs=False,        \n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "if client.indices.exists(index_name):\n",
    "\n",
    "    resp = client.indices.open(index = index_name)\n",
    "    print(resp)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX SETTINGS')\n",
    "    settings = client.indices.get_settings(index = index_name)\n",
    "    pprint(settings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX MAPPINGS')\n",
    "    mappings = client.indices.get_mapping(index = index_name)\n",
    "    pprint(mappings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX #DOCs')\n",
    "    print(client.count(index = index_name))\n",
    "else:\n",
    "    print(\"Index does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.delete(index=index_name, ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the index mappings for video captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"number_of_replicas\": 0,\n",
    "            \"number_of_shards\": 4,\n",
    "            \"refresh_interval\": \"-1\",\n",
    "            \"knn\": \"true\"\n",
    "        },\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"strict\",\n",
    "        \"properties\": {\n",
    "            #video_id\n",
    "            \"title\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            #sentences\n",
    "            \"description\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    print(\"Index already exists.\")\n",
    "else:        \n",
    "    response = client.indices.create(index_name, body=index_body)\n",
    "    print('\\nCreating index...')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_id, data in final_dataset_captions.items():\n",
    "    print(f\"Title: {video_id}\")\n",
    "    print(f\"Description: {data['segments']['sentences']}\")\n",
    "\n",
    "for video_id, data in final_dataset_captions.items():\n",
    "    filtered_caption = {\n",
    "        \"title\": video_id,\n",
    "        \"description\": data['segments']['sentences']\n",
    "    }\n",
    "    \n",
    "    resp = client.index(index=index_name, id=video_id, body=filtered_caption)\n",
    "    print(resp['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-based Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.count(index = index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtxt = \"finally gets a spare.\"\n",
    "\n",
    "text_query = {\n",
    "  \"size\": 5,\n",
    "  \"_source\": ['title', 'description'],\n",
    "  \"query\": {\n",
    "    \"multi_match\": {\n",
    "      \"query\": qtxt,\n",
    "      \"fields\": ['description'],\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body=text_query,\n",
    "    index=index_name\n",
    ")\n",
    "\n",
    "print(\"\\nSearch results:\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term-level Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_query = {\n",
    "    \"size\": 5,\n",
    "    \"_source\": [\"title\", \"description\"],\n",
    "    \"query\": {\n",
    "        \"term\": {\n",
    "            \"title\": \"v_od9EdcDcByA\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body=term_query,\n",
    "    index=index_name\n",
    ")\n",
    "\n",
    "print(\"\\nSearch results:\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_query = {\n",
    "    \"size\": 5,\n",
    "    \"_source\": [\"title\", \"description\"],\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"match\": {\"description\": \"skate\"}},  # Must contain \"skate\"\n",
    "                {\"match\": {\"description\": \"tricks\"}}  # Must contain \"tricks\"\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body=bool_query,\n",
    "    index=index_name\n",
    ")\n",
    "\n",
    "print(\"\\nSearch results:\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Embeddings Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.delete(index=index_name, ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Index mappings to support k-nn vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"number_of_replicas\": 0,\n",
    "            \"number_of_shards\": 4,\n",
    "            \"refresh_interval\": \"-1\",\n",
    "            \"knn\": True\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"strict\",\n",
    "        \"properties\": {\n",
    "            \"title\": { \"type\": \"keyword\" },\n",
    "            \"description\": { \"type\": \"text\" },\n",
    "            \"sentence_embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"innerproduct\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 256,\n",
    "                        \"m\": 48\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "if client.indices.exists(index=index_name):\n",
    "    print(\"Index already existed. You may force the new mappings.\")\n",
    "else:        \n",
    "    response = client.indices.create(index_name, body=index_body)\n",
    "    print('\\nCreating index:')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.count(index = index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take average of all tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the video text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = {}\n",
    "\n",
    "for video_id, data in final_dataset_captions.items():\n",
    "    # Join all sentences to one paragraph-like string\n",
    "    full_description = \" \".join(data['segments']['sentences'])\n",
    "    \n",
    "    # Encode the full description\n",
    "    embedding = encode(full_description)\n",
    "    \n",
    "    all_embeddings[video_id] = {\n",
    "        \"title\": video_id,\n",
    "        \"description\": data['segments']['sentences'],\n",
    "        \"sentence_embedding\": embedding[0].numpy()\n",
    "    }\n",
    "        \n",
    "    resp = client.index(index=index_name, id=video_id, body=all_embeddings[video_id])\n",
    "    print(resp['result'])\n",
    "\n",
    "    stored = client.get(index=index_name, id=video_id)\n",
    "    print(\"\\nIndexed Document:\")\n",
    "    pprint(stored[\"_source\"])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Save the data to the pickle file\n",
    "with open('all_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('all_embeddings.pkl', 'rb') as f:\n",
    "    all_embeddings = pickle.load(f)\n",
    "print(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search query for embedding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.refresh(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"finally gets a spare.\"\n",
    "query_emb = encode(query)\n",
    "\n",
    "query_denc = {\n",
    "  'size': 5,\n",
    "  '_source': ['title', 'description'],\n",
    "   \"query\": {\n",
    "        \"knn\": {\n",
    "          \"sentence_embedding\": {\n",
    "            \"vector\": query_emb[0].numpy(),\n",
    "            \"k\": 2\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "}\n",
    "\n",
    "response = client.search(\n",
    "    body = query_denc,\n",
    "    index = index_name\n",
    ")\n",
    "\n",
    "print('\\nSearch results:')\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss how the embeddings space organize data and allow for specific search:\n",
    "\n",
    "### 1. Data Organization in Embedding Space\n",
    "In **semantic search**, each document is transformed into a vector and positioned within a **high-dimensional space**. Documents with similar meanings are placed **close together**, regardless of the actual words used.\n",
    "\n",
    "In **traditional search**, documents are stored based on **text terms and frequencies**. The index organizes data around **keyword occurrence**, not meaning.\n",
    "\n",
    "### 2. Proximity-Based Search\n",
    "In the **embedding space**, search is performed by finding documents that are **nearest neighbors** to the query vector. This means results are selected based on **semantic similarity**, not shared vocabulary.\n",
    "\n",
    "In **traditional search**, relevance is determined by **keyword overlap and frequency**. The system doesn’t consider whether two different words or phrases mean the same thing.\n",
    "\n",
    "### 3. Flexibility in Querying\n",
    "**Semantic search** enables queries to return documents based on the **meaning alignment**, even if no keywords match directly.\n",
    "\n",
    "**Traditional search** would require the exact terms to appear in the document to be considered relevant, limiting its ability to understand **context or nuance**.\n",
    "\n",
    "### 4. Dimensional Context\n",
    "The **embedding space** captures **contextual relationships** across hundreds of dimensions. This allows it to distinguish between similar words used in **different contexts** (e.g., `\"apple\"` the fruit vs. `\"Apple\"` the company).\n",
    "\n",
    "**Traditional indexing** does not capture this level of nuance—it treats words more **literally and statically**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Constrained Embedding Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "query = \"show me moments of a man surfing.\"\n",
    "query_embedding = encode(query)[0].numpy().tolist()\n",
    "\n",
    "def get_nouns_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc if token.pos_ in {\"NOUN\", \"VERB\"}])\n",
    "\n",
    "filter_terms = get_nouns_verbs(query)\n",
    "print(f\"Filter terms: {filter_terms}\")\n",
    "\n",
    "search_body = {\n",
    "    'size': 100,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [{\n",
    "                \"knn\": {\n",
    "                    \"sentence_embedding\": {\n",
    "                        \"vector\": query_embedding,\n",
    "                        \"k\": 1\n",
    "                    }\n",
    "                }\n",
    "            }],\n",
    "            \"filter\": [{\n",
    "                \"match\": {\n",
    "                    \"description\": {\n",
    "                        \"query\": filter_terms,\n",
    "                        \"operator\": \"or\"\n",
    "                    }\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    },\n",
    "    \"_source\": [\"title\", \"description\"]\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=search_body)\n",
    "print(f\"Total documents found: {response['hits']['total']['value']}\")\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Title: {hit['_source']['title']}\")\n",
    "    print(\"Relevant sentences:\")\n",
    "    for sentence in hit['_source']['description']:\n",
    "        if \"surf\" in sentence.lower():\n",
    "            print(f\"  → {sentence}\")\n",
    "        else:\n",
    "            print(f\"  - {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"bokling\"  # Intentional typo for \"bowling\"\n",
    "\n",
    "search_body = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"description\": {\n",
    "                \"query\": query,\n",
    "                \"fuzziness\": \"AUTO\",  # Automatically corrects \"bokling\" to \"bowling\"\n",
    "                \"operator\": \"or\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=search_body)\n",
    "print(f\"Total documents found: {response['hits']['total']['value']}\")\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(f\"Title: {hit['_source']['title']}\")\n",
    "    print(\"Full description:\")\n",
    "    for sentence in hit['_source']['description']:\n",
    "        print(f\"  - {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Contextual embeddings and Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"He raises his hands feeling victorious.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'nboost/pt-bert-base-uncased-msmarco'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path,  output_hidden_states=True, output_attentions=True)  \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors='pt', add_special_tokens=True, max_length=512, padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "input_tokens_list = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The format is as follow:\n",
    "# outputs['hidden_states'][layer_m][0][token_n]\n",
    "layer_m = 12\n",
    "token_n = 1\n",
    "# Get all the embeddings of one layer:\n",
    "output_embeddings = outputs['hidden_states'][layer_m][0]\n",
    "output_embeddings.shape\n",
    "\n",
    "hidden_states = outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scatterplot(data, words):\n",
    "\n",
    "    if data.shape[1] == 2:\n",
    "        twodim = data\n",
    "    else:\n",
    "        pca = PCA()\n",
    "        pca.fit(output_embeddings.detach().numpy())\n",
    "        twodim = pca.transform(data)[:,:2]\n",
    "    \n",
    "    plt.style.use('default') # https://matplotlib.org/3.5.1/gallery/style_sheets/style_sheets_reference.html\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "\n",
    "    return\n",
    "\n",
    "display_scatterplot(output_embeddings.detach().numpy(), input_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "\n",
    "rows = 3\n",
    "cols = 4\n",
    "fig, ax_full = plt.subplots(rows, cols)\n",
    "fig.set_figheight(rows*4)\n",
    "fig.set_figwidth(cols*4+3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "\n",
    "layer = 0\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "       \n",
    "        ax = ax_full[r,c]\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 10})\n",
    "        current_hidden_state = hidden_states[layer][0].detach().numpy()\n",
    "        \n",
    "        if current_hidden_state.shape[1] == 2:\n",
    "            twodim = current_hidden_state\n",
    "        else:\n",
    "            twodim = PCA().fit_transform(current_hidden_state)[:,:2]\n",
    "\n",
    "        plt.style.use('default') # https://matplotlib.org/3.5.1/gallery/style_sheets/style_sheets_reference.html\n",
    "        im = ax.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "        for word, (x,y) in zip(input_tokens_list, twodim):\n",
    "            ax.text(x+0.05, y+0.05, word[1:])\n",
    "        \n",
    "        # Show all ticks and label them with the respective list entries\n",
    "        ax.set_title(\"Layer \" + str(layer))\n",
    "            \n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        layer = layer + 1\n",
    "\n",
    "fig.suptitle(\"Visualization of all output embeddings from all layers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Observations\n",
    "\n",
    "1. **Layer 0: Initial Embeddings (Non-contextualized)**\n",
    "   - Tokens are close together and not semantically distinct.\n",
    "   - This layer contains basic wordpiece and position embeddings before any attention is applied.\n",
    "\n",
    "2. **Middle Layers (Layers 3–7): Emerging Context**\n",
    "   - Tokens begin to spread out.\n",
    "   - Contextual differences become visible — e.g., `\"victorious\"` starts to drift away from `\"hands\"` or `\"his\"`.\n",
    "   - The model begins to shape representations based on surrounding words.\n",
    "\n",
    "3. **Deeper Layers (Layers 8–11): Fully Contextualized Embeddings**\n",
    "   - Tokens are well-separated and semantically meaningful.\n",
    "   - Embeddings reflect deep contextual understanding (e.g., `\"hands\"` vs. `\"victorious\"` are clearly distinct).\n",
    "   - Special tokens like `[CLS]` and `[SEP]` are positioned separately, indicating their functional role in the architecture.\n",
    "\n",
    "4. **Token Stability vs. Drift**\n",
    "   - Some tokens (e.g., `\"he\"`, `\"hands\"`) move significantly across layers.\n",
    "   - Special tokens (e.g., `[SEP]`) remain relatively stable, as their role is consistent.\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "- The embeddings start out **non-contextualized** and become **increasingly informed by sentence context** as we move up the layers.\n",
    "- This layered transformation allows BERT to encode rich semantic relationships, which are crucial for tasks like classification, ranking, or semantic search.\n",
    "- Visualizing these embeddings reveals how each layer contributes to building up meaning, culminating in task-specific representations in the final layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'nboost/pt-bert-base-uncased-msmarco'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path,  output_hidden_states=True, output_attentions=True)  \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors='pt', add_special_tokens=True, max_length=512, padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "input_tokens_list = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The format is as follow:\n",
    "# outputs['hidden_states'][layer_m][0][token_n]\n",
    "layer_m = 12\n",
    "token_n = 1\n",
    "# Get all the embeddings of one layer:\n",
    "output_embeddings = outputs['hidden_states'][layer_m][0]\n",
    "output_embeddings.shape\n",
    "\n",
    "hidden_states = outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scatterplot(data, words):\n",
    "\n",
    "    if data.shape[1] == 2:\n",
    "        twodim = data\n",
    "    else:\n",
    "        pca = PCA()\n",
    "        pca.fit(output_embeddings.detach().numpy())\n",
    "        twodim = pca.transform(data)[:,:2]\n",
    "    \n",
    "    plt.style.use('default') # https://matplotlib.org/3.5.1/gallery/style_sheets/style_sheets_reference.html\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "\n",
    "    return\n",
    "\n",
    "display_scatterplot(output_embeddings.detach().numpy(), input_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "\n",
    "rows = 3\n",
    "cols = 4\n",
    "fig, ax_full = plt.subplots(rows, cols)\n",
    "fig.set_figheight(rows*4)\n",
    "fig.set_figwidth(cols*4+3)\n",
    "plt.rcParams.update({'font.size': 6})\n",
    "\n",
    "\n",
    "layer = 0\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "       \n",
    "        ax = ax_full[r,c]\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 10})\n",
    "        current_hidden_state = hidden_states[layer][0].detach().numpy()\n",
    "        \n",
    "        if current_hidden_state.shape[1] == 2:\n",
    "            twodim = current_hidden_state\n",
    "        else:\n",
    "            twodim = PCA().fit_transform(current_hidden_state)[:,:2]\n",
    "\n",
    "        plt.style.use('default') # https://matplotlib.org/3.5.1/gallery/style_sheets/style_sheets_reference.html\n",
    "        im = ax.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "        for word, (x,y) in zip(input_tokens_list, twodim):\n",
    "            ax.text(x+0.05, y+0.05, word[1:])\n",
    "        \n",
    "        # Show all ticks and label them with the respective list entries\n",
    "        ax.set_title(\"Layer \" + str(layer))\n",
    "            \n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        layer = layer + 1\n",
    "\n",
    "fig.suptitle(\"Visualization of all output embeddings from all layers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Observations\n",
    "\n",
    "1. **Token Uniqueness via Position**\n",
    "   - Even though all tokens are `\"hello\"`, their embeddings differ.\n",
    "   - This difference is due to **positional encodings** added before the first layer.\n",
    "\n",
    "2. **Early Layer Behavior**\n",
    "   - In Layer 0, the tokens already diverge — showing that **positional encoding is added at the embedding stage**.\n",
    "\n",
    "3. **Later Layers Reveal Positional Structure**\n",
    "   - From Layer 8 onward, tokens are organized in **structured arcs**, reflecting their **relative positions** in the sentence.\n",
    "\n",
    "4. **Special Tokens Are Isolated**\n",
    "   - `[CLS]` and `[SEP]` tokens are consistently separated, showing they are **functionally unique**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Transformers successfully encode **position** via embeddings, even for identical tokens. Over layers, the network maintains and enhances this information, enabling it to understand **word order and relative position** — a crucial property for handling natural language sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_html():\n",
    "  import IPython\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min\",\n",
    "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"He raises his hands feeling victorious.\"\n",
    "question = \"What did he raise?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'nboost/pt-bert-base-uncased-msmarco'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path,  output_hidden_states=True, output_attentions=True)  \n",
    "model = AutoModel.from_pretrained(model_path, config=config)\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors='pt', add_special_tokens=True, max_length=512, padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "attentions = outputs.attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens specific visualization of self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_html()\n",
    "head_view(attentions, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of all the attention heads in one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 1\n",
    "\n",
    "rows = 3\n",
    "cols = 4\n",
    "fig, ax_full = plt.subplots(rows, cols)\n",
    "fig.set_figheight(rows*6)\n",
    "fig.set_figwidth(cols*6+4)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "j = 0\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "       \n",
    "        ax = ax_full[r,c]\n",
    "        \n",
    "        sattention = attentions[layer][0][j].numpy()\n",
    "        sattention = np.flip(sattention, 0)\n",
    "        \n",
    "        plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "        im = ax.pcolormesh(sattention, cmap='gnuplot')\n",
    "\n",
    "        # Show all ticks and label them with the respective list entries\n",
    "        ax.set_title(\"Head \" + str(j))\n",
    "        ax.set_yticks(np.arange(len(tokens)))\n",
    "        if c == 0:\n",
    "            ax.set_yticklabels(reversed(tokens))\n",
    "            ax.set_ylabel(\"Queries\")\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        ax.set_xticks(np.arange(len(tokens)))\n",
    "        if r == rows-1:\n",
    "            ax.set_xticklabels(tokens)\n",
    "            ax.set_xlabel(\"Keys\")\n",
    "            \n",
    "            # Rotate the tick labels and set their alignment.\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                     rotation_mode=\"anchor\")\n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "\n",
    "            \n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        j = j + 1\n",
    "\n",
    "fig.suptitle(\"Layer\" + str(layer) + \" Multi-head Self-attentions\")\n",
    "cbar = fig.colorbar(im, ax=ax_full, location='right', shrink=0.5)\n",
    "cbar.ax.set_ylabel(\"Selt-attention\", rotation=-90, va=\"bottom\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualzation of Self-Attention in all layers and heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_view(attentions, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_a = tokenizer(question, return_tensors=\"pt\")\n",
    "inputs_b = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_a = model(**inputs_a).last_hidden_state.squeeze(0)\n",
    "    output_b = model(**inputs_b).last_hidden_state.squeeze(0) \n",
    "\n",
    "output_a_norm = torch.nn.functional.normalize(output_a, p=2, dim=1)\n",
    "output_b_norm = torch.nn.functional.normalize(output_b, p=2, dim=1)\n",
    "\n",
    "similarity_matrix = torch.matmul(output_a_norm, output_b_norm.T)  # (seq_len_a, seq_len_b)\n",
    "\n",
    "tokens_a = tokenizer.convert_ids_to_tokens(inputs_a[\"input_ids\"][0])\n",
    "tokens_b = tokenizer.convert_ids_to_tokens(inputs_b[\"input_ids\"][0])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(similarity_matrix.numpy(), xticklabels=tokens_b, yticklabels=tokens_a, cmap=\"viridis\", annot=True, fmt=\".2f\")\n",
    "plt.xlabel(\"Sentence Tokens\")\n",
    "plt.ylabel(\"Question Tokens\")\n",
    "plt.title(\"Similarity between Question and Sentence Tokens\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Encoder\n",
    "\n",
    "In the cross-encoder, **all tokens in the input sequence are encoded together**, allowing **token-level interactions** via the self-attention mechanism.\n",
    "\n",
    "**Key Observations:**\n",
    "- Attention heatmaps clearly show **which tokens attend to which others**.\n",
    "- Special tokens like `[CLS]` often attend to semantically rich tokens (e.g., \"victorious\", \"hands\").\n",
    "- Later layers show **sharper, more focused attention** — indicating higher-level understanding.\n",
    "- Attention heads behave differently — some focus on syntax (e.g., function words), others on semantics (e.g., verbs or sentiment words).\n",
    "\n",
    "**Conclusion**: Cross-encoders capture **fine-grained relationships** between words, making them highly effective for tasks requiring deep understanding, like natural language inference or question answering.\n",
    "\n",
    "---\n",
    "\n",
    "#### Dual Encoder\n",
    "\n",
    "In the dual encoder, **each sentence is encoded independently**, without any interaction at the token level.\n",
    "\n",
    "**Key Observations:**\n",
    "- No attention is computed between tokens across sentences — each input is processed in isolation.\n",
    "- No attention heatmaps can be generated between sentence pairs.\n",
    "- Only final sentence embeddings (pooled outputs) are compared — typically using **cosine similarity**.\n",
    "- The model prioritizes **speed and scalability** over fine-grained interaction.\n",
    "\n",
    "**Conclusion**: Dual encoders are ideal for **semantic similarity and retrieval tasks**, where you need fast comparison of many sentences — but they lack interpretability and detailed attention dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "#### Multi-Head Self-Attention using Cross-Encoder\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "- **Self-Attention Dominance**: Most heads (e.g., Head 0, Head 4) show strong diagonal patterns, indicating tokens primarily attend to themselves.\n",
    "  \n",
    "- **Special Token Behavior**:\n",
    "  - `[CLS]` tokens (Heads 0, 3, 6) attend broadly, aggregating global sentence information.\n",
    "  - `[SET]` tokens (Heads 2, 5) focus heavily on \"victorious\", suggesting a role in capturing sentence-final semantics.\n",
    "\n",
    "- **Action-Emotion Links**:\n",
    "  - Heads 1 and 6 connect \"raises\" to \"hands\" (action-object) and \"feeling\" to \"victorious\" (emotion-result).\n",
    "\n",
    "- **Underutilized Heads**: Heads 9–11 exhibit weak/no meaningful attention, potentially redundant for this input.\n",
    "\n",
    "**Conclusion**: The model distributes attention across heads to balance local token relationships (e.g., verb-noun pairs) and global sentence context (via `[CLS]`/`[SET]`). While some heads specialize in semantic or structural roles, others contribute minimally, suggesting room for optimization.\n",
    "\n",
    "---\n",
    "\n",
    "#### Token-Level Similarity in Dual Encoders\n",
    "\n",
    "Although dual encoders do not provide cross-token attention, we can still analyze **semantic alignment between tokens** using cosine similarity between token embeddings of each sentence.\n",
    "\n",
    "**Key Observations:**\n",
    "- Pronouns such as `\"he\"` in the question tend to align closely with `\"he\"` in the sentence, indicating proper referential understanding.\n",
    "- Verbs like `\"raise\"` show strong similarity with their conjugated forms like `\"raises\"`, reflecting the model's ability to capture morphological relationships.\n",
    "- `[CLS]` tokens are highly similar, as they both represent global sentence-level meaning.\n",
    "- Punctuation and structural tokens like `[SEP]` and `\".\"` often have low similarity with semantic tokens, though they may align with each other due to similar positional roles.\n",
    "\n",
    "**Conclusion**: While dual encoders do not compute attention between sequences, analyzing token embedding similarity reveals that the model still encodes meaningful semantic relationships. This can provide interpretability at a coarse level, especially useful for understanding what drives the final sentence embedding similarity.\n",
    "\n",
    "---\n",
    "\n",
    "#### Critical Comparison\n",
    "\n",
    "| Feature                        | Cross-Encoder                  | Dual Encoder                        |\n",
    "|-------------------------------|--------------------------------|-------------------------------------|\n",
    "| Token-level attention         | Yes                            | No                                  |\n",
    "| Inter-sentence interaction    | Yes                            | No                                  |\n",
    "| Interpretability (heatmaps)   | High                           | Limited (via cosine similarity)     |\n",
    "| Computational cost            | Higher                         | Lower                               |\n",
    "| Use case                      | QA, NLI, classification        | Search, retrieval, ranking          |\n",
    "| Representation granularity    | Fine-grained (token-level)     | Coarse (sentence-level only)        |\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Thoughts\n",
    "\n",
    "- **Cross-encoders** provide rich interpretability through attention maps and capture deep contextual relationships — but are computationally expensive.\n",
    "- **Dual encoders** are lightweight and fast, making them practical for large-scale applications — but offer no visibility into token-level interactions.\n",
    "- The choice between them depends on the **task requirements**: precision vs. speed, interpretability vs. scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 11  # analyze the final layer\n",
    "attentions = outputs.attentions  # shape: (num_layers, batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "# Get shape info\n",
    "num_layers = len(attentions)\n",
    "seq_len = attentions[0].shape[-1]\n",
    "\n",
    "# Store total attention received for each token across all heads, per layer\n",
    "attention_received = torch.zeros((num_layers, seq_len))\n",
    "\n",
    "# Loop through each layer\n",
    "for l in range(num_layers):\n",
    "    attn = attentions[l][0]  # shape: (num_heads, seq_len, seq_len)\n",
    "    attn_sum = attn.sum(dim=0)  # sum over query dimension → shape: (seq_len, seq_len)\n",
    "    attention_received[l] = attn_sum.sum(dim=0)  # sum over key dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for token_idx in range(seq_len):\n",
    "    plt.plot(range(num_layers), attention_received[:, token_idx], label=tokens[token_idx])\n",
    "\n",
    "plt.title(\"Total Attention Received by Each Token Across Layers\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Total Attention Received\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability: Attention Received by Tokens Across Layers\n",
    "\n",
    "The plot shows how much **total attention each token receives** across the 12 transformer layers for the sentence:\n",
    "\n",
    "> **\"He raises his hands feeling victorious.\"**\n",
    "\n",
    "Each line represents a token, and its value at each layer indicates how much attention it received from all other tokens combined (summed across all attention heads).\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Observations\n",
    "\n",
    "1. **[CLS] Token Behavior**\n",
    "   - Receives a significant amount of attention in **early (Layers 0–2)** and **late (Layers 10–11)** stages.\n",
    "   - This aligns with its role as the **summary token**, aggregating sentence-level meaning for classification tasks.\n",
    "\n",
    "2. **[SEP] Token Behavior**\n",
    "   - Dominates the middle layers (especially **Layers 4–9**) in terms of attention received.\n",
    "   - This might reflect its role in separating segments or as a structural marker the model relies on to anchor information.\n",
    "\n",
    "3. **\"he\" Token Stands Out**\n",
    "   - The token `\"he\"` receives unusually high attention in **Layers 5–9**, surpassing even structural tokens.\n",
    "   - This suggests the model identifies `\"he\"` as a central semantic component, possibly due to it being the subject of the sentence.\n",
    "\n",
    "4. **Content Words (e.g., \"hands\", \"victorious\")**\n",
    "   - Receive **moderate and stable attention** throughout the layers.\n",
    "   - Tokens like `\"victorious\"` and `\"raises\"` show some variance, indicating they contribute to the evolving meaning but aren’t the primary focus of attention.\n",
    "\n",
    "5. **Function Words (e.g., \"his\", \"feeling\", \".\")**\n",
    "   - These tokens receive consistently **low attention**, showing they are less critical in shaping the final output.\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "- Tokens that are **structurally important** (`[CLS]`, `[SEP]`) and **semantically central** (`\"he\"`) receive the **highest total attention**.\n",
    "- The model learns to focus its attention **dynamically across layers**, initially on structure, then content, and finally back to the `[CLS]` token for producing the final output.\n",
    "- This behavior confirms that attention received is a strong indicator of **token importance**, which helps explain **how the model constructs meaning** layer by layer.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-cv-ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
